{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# What is Deep Learning?\n",
        "\n",
        "Deep Learning is:\n",
        "- A **subset of Machine Learning**.\n",
        "- Uses **neural networks with multiple layers** (deep architectures).\n",
        "- Learns **features + decision boundaries directly from data**.\n",
        "\n",
        "---\n",
        "\n",
        "##  Why \"Deep\"?\n",
        "\n",
        "- \"Deep\" = **many hidden layers** between input and output.\n",
        "- More layers → ability to learn **complex patterns and representations**.\n",
        "\n",
        "---\n",
        "\n",
        "##  Key Components\n",
        "\n",
        "###  Neural Networks:\n",
        "- Composed of **neurons (nodes)** connected by **weights**.\n",
        "- Layers:\n",
        "  - **Input Layer:** Takes features.\n",
        "  - **Hidden Layers:** Extract patterns.\n",
        "  - **Output Layer:** Provides predictions.\n",
        "\n",
        "###  Activation Functions:\n",
        "- Add **non-linearity**.\n",
        "- Common:\n",
        "  - `relu`\n",
        "  - `sigmoid`\n",
        "  - `tanh`\n",
        "  - `softmax` (for multi-class classification)\n",
        "\n",
        "###  Loss Functions:\n",
        "- Measure **prediction error**.\n",
        "- Examples:\n",
        "  - `categorical_crossentropy` (multi-class)\n",
        "  - `binary_crossentropy` (binary)\n",
        "  - `mse` (regression)\n",
        "\n",
        "### 4 Optimizers:\n",
        "- Update weights to minimize loss.\n",
        "- Examples:\n",
        "  - `SGD`\n",
        "  - `Adam`\n",
        "  - `RMSprop`\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "tXfhSB0jVvA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Loss Functions\n",
        "\n",
        "Loss functions **measure how well your model predictions match true labels**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Classification:**\n",
        "\n",
        "- **Binary Classification:**\n",
        "  - `binary_crossentropy`\n",
        "  - Formula:\n",
        "    $$\n",
        "    L = - [y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y})]\n",
        "    $$\n",
        "- **Multi-class Classification:**\n",
        "  - `categorical_crossentropy` (one-hot labels)\n",
        "  - `sparse_categorical_crossentropy` (integer labels)\n",
        "\n",
        "---\n",
        "\n",
        "### **Regression:**\n",
        "\n",
        "- **Mean Squared Error (MSE):**\n",
        "  - Measures average squared difference.\n",
        "  - $$\n",
        "    L = \\frac{1}{n} \\sum_{i} (y_i - \\hat{y}_i)^2\n",
        "    $$\n",
        "- **Mean Absolute Error (MAE):**\n",
        "  - Measures average absolute difference.\n",
        "\n",
        "---\n",
        "\n",
        "##  Gradient Descent (GD)\n",
        "\n",
        "Gradient Descent:\n",
        "- An optimization algorithm to **minimize the loss**.\n",
        "- Computes:\n",
        "  $$\n",
        "  w := w - \\eta \\frac{\\partial L}{\\partial w}\n",
        "  $$\n",
        "  where:\n",
        "  - $ w $ = parameters/weights\n",
        "  - $ \\eta $ = learning rate\n",
        "  - $ \\frac{\\partial L}{\\partial w} $ = gradient of loss\n",
        "\n",
        "---\n",
        "\n",
        "### Key Points:\n",
        "* Takes **steps in the negative gradient direction**.  \n",
        "* Repeats until convergence (loss stops decreasing).\n",
        "\n",
        "---\n",
        "\n",
        "## Stochastic Gradient Descent (SGD)\n",
        "\n",
        "SGD:\n",
        "- Variant of GD.\n",
        "- Updates weights using **one data sample at a time** (or small batches).\n",
        "\n",
        "---\n",
        "\n",
        "### **Difference from GD:**\n",
        "| GD | SGD |\n",
        "|----|-----|\n",
        "| Uses **all data** per update | Uses **one sample** per update |\n",
        "| Stable but slow | Faster updates, more noise |\n",
        "| Needs large memory | Memory efficient |\n",
        "\n",
        "---\n",
        "\n",
        "### **Mini-batch SGD:**\n",
        "Uses **small batches (e.g., 32, 64 samples)** per update:\n",
        "Balance between stability (GD) and speed (SGD).\n",
        "\n",
        "---\n",
        "\n",
        "##  Why SGD is used in Deep Learning?\n",
        "\n",
        "1. Faster convergence on large datasets.  \n",
        "2. Adds noise to help escape local minima.  \n",
        "3. Scalable for large models and data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_bkOnFKQWjmS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Negative Log-Likelihood (NLL)\n",
        "\n",
        "\n",
        "\n",
        "##  What is it?\n",
        "\n",
        "**Negative Log-Likelihood (NLL)** is a **loss function** that measures how well predicted probabilities match true labels.\n",
        "\n",
        "---\n",
        "\n",
        "##  Formula\n",
        "\n",
        "For one-hot labels:\n",
        "$$\n",
        "L = - \\sum_{i} y_i \\cdot \\log(\\hat{y}_i)\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $ y_i $ = true label (1 for correct class, 0 otherwise),\n",
        "- $ \\hat{y}_i $ = predicted probability for class $ i $.\n",
        "\n",
        " **Same as `categorical_crossentropy`** in multi-class classification.\n",
        "\n",
        "---\n",
        "\n",
        "##  Why use NLL?\n",
        "\n",
        " 1. Penalizes **low probabilities for correct labels heavily**.  \n",
        " 2. Encourages **high confidence for correct predictions**.  \n",
        " 3. Smooth, differentiable, ideal for **gradient-based optimization**.\n",
        "\n",
        "---\n",
        "\n",
        "##  Where used?\n",
        "\n",
        "- **Classification tasks**\n",
        "- **Language models**\n",
        "- Any **probabilistic deep learning task requiring likelihood maximization**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        " >> Minimize NLL → **Maximize your model's confidence on correct classes.**\n",
        "\n"
      ],
      "metadata": {
        "id": "DSp6Inc4XkeX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning Rate, Momentum, Dropout, and Regularization\n",
        "\n",
        "---\n",
        "\n",
        "##  Learning Rate (LR)\n",
        "\n",
        "- Controls **step size** during optimization.\n",
        "- Too high → diverges.\n",
        "- Too low → slow convergence.\n",
        "- Typical values: `0.1`, `0.01`, `0.001`.\n",
        "\n",
        " Tune using learning rate schedules or optimizers like `Adam` which adapt LR automatically.\n",
        "\n",
        "---\n",
        "\n",
        "##  Momentum\n",
        "\n",
        "- Helps **accelerate gradients in relevant directions** and **dampens oscillations**.\n",
        "- Adds a fraction of previous update to the current update:\n",
        "  $$\n",
        "  v_t = \\beta v_{t-1} + (1 - \\beta) \\nabla L\n",
        "  $$\n",
        "- Typical `momentum` values: `0.9`, `0.99`.\n",
        "\n",
        " Useful with `SGD` to speed up convergence.\n",
        "\n",
        "---\n",
        "\n",
        "##  Dropout\n",
        "\n",
        "- **Regularization technique** to prevent overfitting.\n",
        "- Randomly “drops” a fraction of neurons during training.\n",
        "- Forces the network to **learn redundant, robust representations**.\n",
        "\n",
        "✅Typical dropout rates: `0.2`, `0.5`.\n",
        "\n",
        "---\n",
        "\n",
        "##  Regularization\n",
        "\n",
        "Adds a **penalty to the loss function** to prevent overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### **Types:**\n",
        "\n",
        "- **L1 Regularization (Lasso):**\n",
        "  Adds:\n",
        "  $$\n",
        "  \\lambda \\sum |w_i|\n",
        "  $$\n",
        "  Encourages sparsity (many weights → 0).\n",
        "\n",
        "- **L2 Regularization (Ridge):**\n",
        "  Adds:\n",
        "  $$\n",
        "  \\lambda \\sum w_i^2\n",
        "  $$\n",
        "  Encourages smaller weights, smooths the model.\n",
        "\n",
        "- **Elastic Net:**\n",
        "  Combines L1 + L2.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FfE4O9SeY4rS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIawGMA9Vj-r"
      },
      "outputs": [],
      "source": []
    }
  ]
}